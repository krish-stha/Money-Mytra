"""
This type stub file was generated by pyright.
"""

import proto

"""
This type stub file was generated by pyright.
"""
__protobuf__ = ...
class HarmCategory(proto.Enum):
    r"""The category of a rating.

    These categories cover various kinds of harms that developers
    may wish to adjust.

    Values:
        HARM_CATEGORY_UNSPECIFIED (0):
            Category is unspecified.
        HARM_CATEGORY_DEROGATORY (1):
            **PaLM** - Negative or harmful comments targeting identity
            and/or protected attribute.
        HARM_CATEGORY_TOXICITY (2):
            **PaLM** - Content that is rude, disrespectful, or profane.
        HARM_CATEGORY_VIOLENCE (3):
            **PaLM** - Describes scenarios depicting violence against an
            individual or group, or general descriptions of gore.
        HARM_CATEGORY_SEXUAL (4):
            **PaLM** - Contains references to sexual acts or other lewd
            content.
        HARM_CATEGORY_MEDICAL (5):
            **PaLM** - Promotes unchecked medical advice.
        HARM_CATEGORY_DANGEROUS (6):
            **PaLM** - Dangerous content that promotes, facilitates, or
            encourages harmful acts.
        HARM_CATEGORY_HARASSMENT (7):
            **Gemini** - Harassment content.
        HARM_CATEGORY_HATE_SPEECH (8):
            **Gemini** - Hate speech and content.
        HARM_CATEGORY_SEXUALLY_EXPLICIT (9):
            **Gemini** - Sexually explicit content.
        HARM_CATEGORY_DANGEROUS_CONTENT (10):
            **Gemini** - Dangerous content.
        HARM_CATEGORY_CIVIC_INTEGRITY (11):
            **Gemini** - Content that may be used to harm civic
            integrity.
    """
    HARM_CATEGORY_UNSPECIFIED = ...
    HARM_CATEGORY_DEROGATORY = ...
    HARM_CATEGORY_TOXICITY = ...
    HARM_CATEGORY_VIOLENCE = ...
    HARM_CATEGORY_SEXUAL = ...
    HARM_CATEGORY_MEDICAL = ...
    HARM_CATEGORY_DANGEROUS = ...
    HARM_CATEGORY_HARASSMENT = ...
    HARM_CATEGORY_HATE_SPEECH = ...
    HARM_CATEGORY_SEXUALLY_EXPLICIT = ...
    HARM_CATEGORY_DANGEROUS_CONTENT = ...
    HARM_CATEGORY_CIVIC_INTEGRITY = ...


class ContentFilter(proto.Message):
    r"""Content filtering metadata associated with processing a
    single request.
    ContentFilter contains a reason and an optional supporting
    string. The reason may be unspecified.


    .. _oneof: https://proto-plus-python.readthedocs.io/en/stable/fields.html#oneofs-mutually-exclusive-fields

    Attributes:
        reason (google.ai.generativelanguage_v1beta.types.ContentFilter.BlockedReason):
            The reason content was blocked during request
            processing.
        message (str):
            A string that describes the filtering
            behavior in more detail.

            This field is a member of `oneof`_ ``_message``.
    """
    class BlockedReason(proto.Enum):
        r"""A list of reasons why content may have been blocked.

        Values:
            BLOCKED_REASON_UNSPECIFIED (0):
                A blocked reason was not specified.
            SAFETY (1):
                Content was blocked by safety settings.
            OTHER (2):
                Content was blocked, but the reason is
                uncategorized.
        """
        BLOCKED_REASON_UNSPECIFIED = ...
        SAFETY = ...
        OTHER = ...
    
    
    reason: BlockedReason = ...
    message: str = ...


class SafetyFeedback(proto.Message):
    r"""Safety feedback for an entire request.

    This field is populated if content in the input and/or response
    is blocked due to safety settings. SafetyFeedback may not exist
    for every HarmCategory. Each SafetyFeedback will return the
    safety settings used by the request as well as the lowest
    HarmProbability that should be allowed in order to return a
    result.

    Attributes:
        rating (google.ai.generativelanguage_v1beta.types.SafetyRating):
            Safety rating evaluated from content.
        setting (google.ai.generativelanguage_v1beta.types.SafetySetting):
            Safety settings applied to the request.
    """
    rating: SafetyRating = ...
    setting: SafetySetting = ...


class SafetyRating(proto.Message):
    r"""Safety rating for a piece of content.

    The safety rating contains the category of harm and the harm
    probability level in that category for a piece of content.
    Content is classified for safety across a number of harm
    categories and the probability of the harm classification is
    included here.

    Attributes:
        category (google.ai.generativelanguage_v1beta.types.HarmCategory):
            Required. The category for this rating.
        probability (google.ai.generativelanguage_v1beta.types.SafetyRating.HarmProbability):
            Required. The probability of harm for this
            content.
        blocked (bool):
            Was this content blocked because of this
            rating?
    """
    class HarmProbability(proto.Enum):
        r"""The probability that a piece of content is harmful.

        The classification system gives the probability of the content
        being unsafe. This does not indicate the severity of harm for a
        piece of content.

        Values:
            HARM_PROBABILITY_UNSPECIFIED (0):
                Probability is unspecified.
            NEGLIGIBLE (1):
                Content has a negligible chance of being
                unsafe.
            LOW (2):
                Content has a low chance of being unsafe.
            MEDIUM (3):
                Content has a medium chance of being unsafe.
            HIGH (4):
                Content has a high chance of being unsafe.
        """
        HARM_PROBABILITY_UNSPECIFIED = ...
        NEGLIGIBLE = ...
        LOW = ...
        MEDIUM = ...
        HIGH = ...
    
    
    category: HarmCategory = ...
    probability: HarmProbability = ...
    blocked: bool = ...


class SafetySetting(proto.Message):
    r"""Safety setting, affecting the safety-blocking behavior.

    Passing a safety setting for a category changes the allowed
    probability that content is blocked.

    Attributes:
        category (google.ai.generativelanguage_v1beta.types.HarmCategory):
            Required. The category for this setting.
        threshold (google.ai.generativelanguage_v1beta.types.SafetySetting.HarmBlockThreshold):
            Required. Controls the probability threshold
            at which harm is blocked.
    """
    class HarmBlockThreshold(proto.Enum):
        r"""Block at and beyond a specified harm probability.

        Values:
            HARM_BLOCK_THRESHOLD_UNSPECIFIED (0):
                Threshold is unspecified.
            BLOCK_LOW_AND_ABOVE (1):
                Content with NEGLIGIBLE will be allowed.
            BLOCK_MEDIUM_AND_ABOVE (2):
                Content with NEGLIGIBLE and LOW will be
                allowed.
            BLOCK_ONLY_HIGH (3):
                Content with NEGLIGIBLE, LOW, and MEDIUM will
                be allowed.
            BLOCK_NONE (4):
                All content will be allowed.
            OFF (5):
                Turn off the safety filter.
        """
        HARM_BLOCK_THRESHOLD_UNSPECIFIED = ...
        BLOCK_LOW_AND_ABOVE = ...
        BLOCK_MEDIUM_AND_ABOVE = ...
        BLOCK_ONLY_HIGH = ...
        BLOCK_NONE = ...
        OFF = ...
    
    
    category: HarmCategory = ...
    threshold: HarmBlockThreshold = ...


__all__ = tuple(sorted(__protobuf__.manifest))
