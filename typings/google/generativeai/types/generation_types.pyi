"""
This type stub file was generated by pyright.
"""

import contextlib
import dataclasses
import sys
from collections.abc import AsyncIterable, Iterable, Mapping
from typing import Any, Union
from typing_extensions import TypedDict
from google.generativeai import protos, string_utils

"""
This type stub file was generated by pyright.
"""
__all__ = ["AsyncGenerateContentResponse", "BlockedPromptException", "StopCandidateException", "IncompleteIterationError", "BrokenResponseError", "GenerationConfigDict", "GenerationConfigType", "GenerationConfig", "GenerateContentResponse"]
if sys.version_info < (3, 10):
    ...
class BlockedPromptException(Exception):
    ...


class StopCandidateException(Exception):
    ...


class IncompleteIterationError(Exception):
    ...


class BrokenResponseError(Exception):
    ...


class GenerationConfigDict(TypedDict, total=False):
    candidate_count: int
    stop_sequences: Iterable[str]
    max_output_tokens: int
    temperature: float
    response_mime_type: str
    response_schema: protos.Schema | Mapping[str, Any]
    ...


@dataclasses.dataclass
class GenerationConfig:
    """A simple dataclass used to configure the generation parameters of `GenerativeModel.generate_content`.

    Attributes:
        candidate_count:
            Number of generated responses to return.
        stop_sequences:
            The set of character sequences (up
            to 5) that will stop output generation. If
            specified, the API will stop at the first
            appearance of a stop sequence. The stop sequence
            will not be included as part of the response.
        max_output_tokens:
            The maximum number of tokens to include in a
            candidate.

            If unset, this will default to output_token_limit specified
            in the model's specification.
        temperature:
            Controls the randomness of the output. Note: The
            default value varies by model, see the `Model.temperature`
            attribute of the `Model` returned the `genai.get_model`
            function.

            Values can range from [0.0,1.0], inclusive. A value closer
            to 1.0 will produce responses that are more varied and
            creative, while a value closer to 0.0 will typically result
            in more straightforward responses from the model.
        top_p:
            Optional. The maximum cumulative probability of tokens to
            consider when sampling.

            The model uses combined Top-k and nucleus sampling.

            Tokens are sorted based on their assigned probabilities so
            that only the most likely tokens are considered. Top-k
            sampling directly limits the maximum number of tokens to
            consider, while Nucleus sampling limits number of tokens
            based on the cumulative probability.

            Note: The default value varies by model, see the
            `Model.top_p` attribute of the `Model` returned the
            `genai.get_model` function.

        top_k (int):
            Optional. The maximum number of tokens to consider when
            sampling.

            The model uses combined Top-k and nucleus sampling.

            Top-k sampling considers the set of `top_k` most probable
            tokens. Defaults to 40.

            Note: The default value varies by model, see the
            `Model.top_k` attribute of the `Model` returned the
            `genai.get_model` function.
        seed:
            Optional.  Seed used in decoding. If not set, the request uses a randomly generated seed.
        response_mime_type:
            Optional. Output response mimetype of the generated candidate text.

            Supported mimetype:
                `text/plain`: (default) Text output.
                `text/x-enum`: for use with a string-enum in `response_schema`
                `application/json`: JSON response in the candidates.

        response_schema:
            Optional. Specifies the format of the JSON requested if response_mime_type is
            `application/json`.
        presence_penalty:
            Optional.
        frequency_penalty:
            Optional.
        response_logprobs:
            Optional. If true, export the `logprobs` results in response.
        logprobs:
            Optional. Number of candidates of log probabilities to return at each step of decoding.
    """
    candidate_count: int | None = ...
    stop_sequences: Iterable[str] | None = ...
    max_output_tokens: int | None = ...
    temperature: float | None = ...
    top_p: float | None = ...
    top_k: int | None = ...
    seed: int | None = ...
    response_mime_type: str | None = ...
    response_schema: protos.Schema | Mapping[str, Any] | type | None = ...
    presence_penalty: float | None = ...
    frequency_penalty: float | None = ...
    response_logprobs: bool | None = ...
    logprobs: int | None = ...


GenerationConfigType = Union[protos.GenerationConfig, GenerationConfigDict, GenerationConfig]
def to_generation_config_dict(generation_config: GenerationConfigType):
    ...

_INCOMPLETE_ITERATION_MESSAGE = ...
class BaseGenerateContentResponse:
    def __init__(self, done: bool, iterator: (None | Iterable[protos.GenerateContentResponse] | AsyncIterable[protos.GenerateContentResponse]), result: protos.GenerateContentResponse, chunks: Iterable[protos.GenerateContentResponse] | None = ...) -> None:
        ...
    
    def to_dict(self):
        """Returns the result as a JSON-compatible dict.

        Note: This doesn't capture the iterator state when streaming, it only captures the accumulated
        `GenerateContentResponse` fields.

        >>> import json
        >>> response = model.generate_content('Hello?')
        >>> json.dumps(response.to_dict())
        """
        ...
    
    @property
    def candidates(self):
        """The list of candidate responses.

        Raises:
            IncompleteIterationError: With `stream=True` if iteration over the stream was not completed.
        """
        ...
    
    @property
    def parts(self):
        """A quick accessor equivalent to `self.candidates[0].content.parts`

        Raises:
            ValueError: If the candidate list does not contain exactly one candidate.
        """
        ...
    
    @property
    def text(self):
        """A quick accessor equivalent to `self.candidates[0].content.parts[0].text`

        Raises:
            ValueError: If the candidate list or parts list does not contain exactly one entry.
        """
        ...
    
    @property
    def prompt_feedback(self):
        ...
    
    @property
    def usage_metadata(self):
        ...
    
    def __str__(self) -> str:
        ...
    
    __repr__ = ...


@contextlib.contextmanager
def rewrite_stream_error():
    ...

GENERATE_CONTENT_RESPONSE_DOC = ...
ASYNC_GENERATE_CONTENT_RESPONSE_DOC = ...
@string_utils.set_doc(GENERATE_CONTENT_RESPONSE_DOC)
class GenerateContentResponse(BaseGenerateContentResponse):
    @classmethod
    def from_iterator(cls, iterator: Iterable[protos.GenerateContentResponse]):
        ...
    
    @classmethod
    def from_response(cls, response: protos.GenerateContentResponse):
        ...
    
    def __iter__(self):
        ...
    
    def resolve(self):
        ...
    


@string_utils.set_doc(ASYNC_GENERATE_CONTENT_RESPONSE_DOC)
class AsyncGenerateContentResponse(BaseGenerateContentResponse):
    @classmethod
    async def from_aiterator(cls, iterator: AsyncIterable[protos.GenerateContentResponse]):
        ...
    
    @classmethod
    def from_response(cls, response: protos.GenerateContentResponse):
        ...
    
    async def __aiter__(self):
        ...
    
    async def resolve(self):
        ...
    


