"""
This type stub file was generated by pyright.
"""

import google.ai.generativelanguage as glm
from collections.abc import Iterable
from typing import Iterable, Mapping, Optional, Union
from typing_extensions import TypedDict
from google.generativeai import protos
from google.generativeai.types import content_types, helper_types, model_types, retriever_types, safety_types
from google.generativeai.types.retriever_types import MetadataFilter

"""
This type stub file was generated by pyright.
"""
DEFAULT_ANSWER_MODEL = ...
AnswerStyle = protos.GenerateAnswerRequest.AnswerStyle
AnswerStyleOptions = Union[int, str, AnswerStyle]
_ANSWER_STYLES: dict[AnswerStyleOptions, AnswerStyle] = ...
def to_answer_style(x: AnswerStyleOptions) -> AnswerStyle:
    ...

GroundingPassageOptions = ...
GroundingPassagesOptions = Union[protos.GroundingPassages, Iterable[GroundingPassageOptions], Mapping[str, content_types.ContentType],]
SourceNameType = Union[str, retriever_types.Corpus, protos.Corpus, retriever_types.Document, protos.Document]
class SemanticRetrieverConfigDict(TypedDict):
    source: SourceNameType
    query: content_types.ContentsType
    metadata_filter: Optional[Iterable[MetadataFilter]]
    max_chunks_count: Optional[int]
    minimum_relevance_score: Optional[float]
    ...


SemanticRetrieverConfigOptions = Union[SourceNameType, SemanticRetrieverConfigDict, protos.SemanticRetrieverConfig,]
def generate_answer(*, model: model_types.AnyModelNameOptions = ..., contents: content_types.ContentsType, inline_passages: GroundingPassagesOptions | None = ..., semantic_retriever: SemanticRetrieverConfigOptions | None = ..., answer_style: AnswerStyle | None = ..., safety_settings: safety_types.SafetySettingOptions | None = ..., temperature: float | None = ..., client: glm.GenerativeServiceClient | None = ..., request_options: helper_types.RequestOptionsType | None = ...):
    """Calls the GenerateAnswer API and returns a `types.Answer` containing the response.

    You can pass a literal list of text chunks:

    >>> from google.generativeai import answer
    >>> answer.generate_answer(
    ...     content=question,
    ...     inline_passages=splitter.split(document)
    ... )

    Or pass a reference to a retreiver Document or Corpus:

    >>> from google.generativeai import answer
    >>> from google.generativeai import retriever
    >>> my_corpus = retriever.get_corpus('my_corpus')
    >>> genai.generate_answer(
    ...     content=question,
    ...     semantic_retriever=my_corpus
    ... )


    Args:
        model: Which model to call, as a string or a `types.Model`.
        contents: The question to be answered by the model, grounded in the
                provided source.
        inline_passages: Grounding passages (a list of `Content`-like objects or (id, content) pairs,
            or a `protos.GroundingPassages`) to send inline with the request. Exclusive with `semantic_retriever`,
            one must be set, but not both.
        semantic_retriever: A Corpus, Document, or `protos.SemanticRetrieverConfig` to use for grounding. Exclusive with
             `inline_passages`, one must be set, but not both.
        answer_style: Style in which the grounded answer should be returned.
        safety_settings: Safety settings for generated output. Defaults to None.
        temperature: Controls the randomness of the output.
        client: If you're not relying on a default client, you pass a `glm.GenerativeServiceClient` instead.
        request_options: Options for the request.

    Returns:
        A `types.Answer` containing the model's text answer response.
    """
    ...

async def generate_answer_async(*, model: model_types.AnyModelNameOptions = ..., contents: content_types.ContentsType, inline_passages: GroundingPassagesOptions | None = ..., semantic_retriever: SemanticRetrieverConfigOptions | None = ..., answer_style: AnswerStyle | None = ..., safety_settings: safety_types.SafetySettingOptions | None = ..., temperature: float | None = ..., client: glm.GenerativeServiceClient | None = ..., request_options: helper_types.RequestOptionsType | None = ...):
    """
    Calls the API and returns a `types.Answer` containing the answer.

    Args:
        model: Which model to call, as a string or a `types.Model`.
        contents: The question to be answered by the model, grounded in the
                provided source.
        inline_passages: Grounding passages (a list of `Content`-like objects or (id, content) pairs,
            or a `protos.GroundingPassages`) to send inline with the request. Exclusive with `semantic_retriever`,
            one must be set, but not both.
        semantic_retriever: A Corpus, Document, or `protos.SemanticRetrieverConfig` to use for grounding. Exclusive with
             `inline_passages`, one must be set, but not both.
        answer_style: Style in which the grounded answer should be returned.
        safety_settings: Safety settings for generated output. Defaults to None.
        temperature: Controls the randomness of the output.
        client: If you're not relying on a default client, you pass a `glm.GenerativeServiceClient` instead.

    Returns:
        A `types.Answer` containing the model's text answer response.
    """
    ...

